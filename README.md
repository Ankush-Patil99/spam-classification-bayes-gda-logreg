

# ğŸ“§ SMS Spam Classification â€” Naive Bayes, GDA, Logistic Regression
A complete end-to-end SMS spam detection project combining classical
machine-learning models with probability-driven approaches. This project
is clean, professional, and GitHub-ready.

## ğŸš€ PROJECT HIGHLIGHTS:-

âœ” Multinomial Naive Bayes  
âœ” Bernoulli Naive Bayes  
âœ” Gaussian Naive Bayes  
âœ” Gaussian Discriminant Analysis (GDA) â€” implemented from scratch  
âœ” Logistic Regression (benchmark model)  
âœ” TF-IDF text vectorization  
âœ” Strong evaluation: ROC, PR curve, calibration, confusion matrices  
âœ” Feature importance, token importance, misclassification analysis  
âœ” Modular src/ folder for training + prediction scripts  
âœ” Saved models for real-world inference  
---
## ğŸ“‘ Table of Contents
- [ğŸš€ Project Highlights](#-project-highlights)
- [ğŸ“¦ Folder Structure](#-full-project-folder-structure-click-to-expand)
- [ğŸ§¹ Preprocessing Steps](#-preprocessing-steps)
- [ğŸ§  Models Trained](#-models-trained)
  - [1ï¸âƒ£ Naive Bayes Family](#1-naive-bayes-family)
  - [2ï¸âƒ£ Gaussian Discriminant Analysis (GDA)](#2-gaussian-discriminant-analysis-gda--implemented-from-scratch)
  - [3ï¸âƒ£ Logistic Regression](#3-logistic-regression-best-model)
- [ğŸ“ˆ Evaluation Metrics](#-evaluation-metrics)
- [ğŸ–¼ Visualizations Included](#-visualizations-included)
- [ğŸ“˜ Prediction Example](#-prediction-example)
- [ğŸ Summary](#-summary)
- [ğŸ§° Tech Stack](#-tech-stack)
- [ğŸ‘¤ Author](#-author)
---

<details>
<summary><h2>ğŸ“¦ Full Project Folder Structure (Click to Expand)</h2></summary>

<br>

## ğŸ“ models/
-  [final_model.pkl](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/models/final_model.pkl)
-  [tfidf_vectorizer.pkl](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/models/tfidf_vectorizer.pkl)

---

## ğŸ“ notebooks/
- ğŸ““ [spam_classification.ipynb](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/notebooks/spam_classification.ipynb)

---

## ğŸ“ plots/
-  [feature_importance_logreg.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/plots/feature_importance_logreg.png)
-  [final_model_comparison.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/plots/final_model_comparison.png)
-  [multiclassification_examples.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/plots/multiclassification_examples.png)
-  [top_spam_tokens_MNB.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/plots/top_spam_tokens_MNB.png)

---

## ğŸ“ results/
-  [calibration_plot.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/results/calibration_plot.png)
-  [confusion_matrix_logreg.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/results/confusion_matrix_logreg.png)
-  [confusion_matrix_mnb.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/results/confusion_matrix_mnb.png)
-  [model_comparison_results.csv](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/results/model_comparison_results.csv)
-  [precision_recall_curve.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/results/precision_recall_curve.png)
-  [ROC_curve.png](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/results/ROC_curve.png)

---

## ğŸ“ src/
-  [clean_text.py](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/src/clean_text.py)
-  [predict.py](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/src/predict.py)
-  [train_gda.py](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/src/train_gda.py)
-  [train_logreg.py](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/src/train_logreg.py)
-  [train_nb.py](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/src/train_nb.py)
-  [vectorizer.py](https://github.com/Ankush-Patil99/spam-classification-bayes-gda-logreg/blob/main/spam-classification-bayes-gda-logreg/src/vectorizer.py)

</details>
  

## ğŸ§¹ Preprocessing Steps
The SMS messages go through a complete cleaning + vectorization pipeline:

1. **Lowercasing** â€“ normalize text  
2. **URL removal** â€“ remove hyperlinks  
3. **Number removal** â€“ remove digit-heavy tokens  
4. **Punctuation cleaning** â€“ strip special characters  
5. **Stopword removal** â€“ remove common non-informative words  
6. **Token filtering** â€“ remove very short or meaningless tokens  
7. **TF-IDF Vectorization**  
   - Uni-grams + Bi-grams  
   - `max_features = 6000`  
   - `min_df = 2`  
   - Produces a high-quality sparse representation of text  


---

## ğŸ§  Models Trained

### 1. Naive Bayes Family
- **MultinomialNB** â†’ Best for TF-IDF sparse matrices  
- **BernoulliNB** â†’ Binary text features  
- **GaussianNB** â†’ Baseline comparison  

---

###  2. Gaussian Discriminant Analysis (GDA) â€” *Implemented From Scratch*
GDA was implemented manually using the original mathematical formulation:

- Compute **mean vectors** Î¼â‚€, Î¼â‚  
- Compute **shared covariance matrix** Î£  
- Add small regularization (ÎµI)  
- Compute **priors** Ï€â‚€, Ï€â‚  
- Implement discriminant function: **Î´(x) = xáµ€ Î£â»Â¹ Î¼ - Â½ Î¼áµ€ Î£â»Â¹ Î¼ + log(Ï€)**

GDA surprisingly performs well even on dense TF-IDF vectors.

---
###  3. Logistic Regression (Best Model)
- **Highest precision, recall and F1-score**  
- Best **calibrated probability outputs**  
- Highly interpretable coefficients  
- Lightweight and robust for text classification 

## ğŸ“ˆ Evaluation Metrics  
The models were evaluated using multiple classification and probability-quality metrics:

- **Accuracy** â€“ overall correctness  
- **Precision** â€“ how many predicted spam messages were actually spam  
- **Recall** â€“ ability to detect spam messages  
- **F1 Score** â€“ balanced metric between precision and recall  
- **ROCâ€“AUC** â€“ ability to separate classes across thresholds  
- **PRâ€“AUC** â€“ especially useful for imbalanced spam datasets  
- **Calibration Curve** â€“ probability correctness (LogReg performs best)  
- **Confusion Matrices** â€“ detailed class-wise error breakdown  

---

## ğŸ–¼ Visualizations Included  
The project includes rich visual diagnostic plots for performance and interpretability:

- **Confusion Matrices** â†’ MultinomialNB & Logistic Regression  
- **ROC Curve** â†’ classifier separability  
- **Precisionâ€“Recall Curve** â†’ performance on imbalanced data  
- **Calibration Plot** â†’ probability quality comparison  
- **Feature Importance Plot (LogReg)** â†’ top predictive words  
- **Top Spam Tokens (MNB)** â†’ strongest spam indicators  
- **Misclassified Samples Analysis** â†’ understand incorrect predictions  

---  
## ğŸ“˜ PREDICTION EXAMPLE

You can test any SMS message using the `predict.py` script:
```python
from src.predict import load_pipeline, predict_message
model, vect = load_pipeline()
label, prob = predict_message("Congratulations! You won a free gift!")
print(label, prob)
```
Output:
```
spam  (probability â‰ˆ 0.98)
```
This clean interface makes the model easy to integrate with APIs, mobile apps, or automation pipelines.

---
## ğŸ SUMMARY


This project delivers a complete, production-ready classical NLP pipeline:
- Modular codebase (src/) for easy reuse  
- Comprehensive evaluation using ROC, PR curve, calibration, confusion matrices  
- Interpretable models with token importance, feature importance, and misclassification analysis  
- Deployment-ready design with saved models & prediction script  
- Excellent GitHub & resume project, showcasing classical ML mastery and mathematical depth (GDA from scratch)  
---
## ğŸ§° Tech Stack

**Languages**
- ğŸŸ¦ Python 3.10+

**Core Libraries**
- ğŸ§® NumPy  
- ğŸ“Š Pandas  
- ğŸ”¤ Scikit-Learn  
- ğŸ§° SciPy  

**NLP & Text Processing**
- ğŸ”¡ NLTK  
- ğŸ§¾ TF-IDF Vectorizer  

**Visualization**
- ğŸ“ˆ Matplotlib  
- ğŸ¨ Seaborn  

**Model Persistence**
- ğŸ’¾ joblib  

**Environment**
- ğŸ§ª Jupyter Notebook  
- ğŸ—‚ GitHub Repository Structure  

---

## ğŸ‘¤ Author
**Ankush Patil**  
Machine Learning & NLP Engineer  
ğŸ“§ **Email**: ankpatil1203@gmail.com  
ğŸ’¼ **LinkedIn**: www.linkedin.com/in/ankush-patil-48989739a  
ğŸŒ **GitHub**: https://github.com/Ankush-Patil99  

Well-structured by a dedicated ML engineer aiming to master classical + probabilistic learning.
